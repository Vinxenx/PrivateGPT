server:
  env_name: ${APP_ENV:qwq}

llm:
  mode: ollama
  max_new_tokens: 512
  context_window: 3900
  temperature: 0.1     # Die Temperatur des Modells. Ein höherer Wert führt zu kreativeren Antworten.

embedding:
  mode: ollama

ollama:
  llm_model: qwq
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434
  keep_alive: 5m
  tfs_z: 1.0           # Tail Free Sampling reduziert den Einfluss weniger wahrscheinlicher Tokens.
  top_k: 40            # Reduziert die Wahrscheinlichkeit von Unsinn. Höhere Werte = vielfältigere Antworten.
  top_p: 0.9           # Arbeitet mit top-k zusammen. Höhere Werte = vielfältigerer Text.
  repeat_last_n: 64    # Wie weit zurück das Modell schauen soll, um Wiederholungen zu vermeiden.
  repeat_penalty: 1.2  # Wie stark Wiederholungen bestraft werden.
  request_timeout: 120.0

vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant 